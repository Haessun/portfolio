{
  "resumeTitle": {
    "title": "Haesun Joung"
  },

  "information": {
    "name": "정해선",
    "contact": [
      { "id": 0, "name": "Email", "href": "gotjs3841@snu.ac.kr", "isEmail": true },
      { "id": 1, "name": "Github", "href": "https://github.com/Haessun" },
      { "id": 2, "name": "GoogleScholar", "href": "https://scholar.google.com/citations?user=yV8xVKoAAAAJ&hl=en" },
      { "id": 3, "name": "LinkedIn", "href": "https://www.linkedin.com/in/해선-정-481b86175/"}
    ]
  },

  "education": [
    {
      "id": 0,
      "name": "Kaywon Arts High School",
      "period": ["2013. 03", "2016. 02"],
      "description": "Major in Music Composition"
    },
    {
      "id": 1,
      "name": "B.S. in Musicology and Computer Science & Engineering @ Seoul National University",
      "period": ["2016. 03", "2021. 08"]
    },
    {
      "id": 2,
      "name": "M.S. in Intelligence and Information @ Seoul National University",
      "period": ["2021. 09", "2023. 08"],
      "thesis": "Music Auto-tagging in Multimedia Content using Robust Music Representation Learned via Domain Adversarial Training"
    },
    {
      "id": 3,
      "name": "Ph.D. in Intelligence and Information @ Seoul National University",
      "period": ["2023. 09", "Recent"],
      "researchArea": "Music Captioning via Reinforcement Learning, Musical Instrument/Timbre Retrieval, Music HCI"
    }
  ],

  "workExperience": {
    "id": 0,
    "name": "Research Intern @ NAVER VIBE AI",
    "period": ["2025. 01", "2025. 02"],
    "description": "Music genre auto-tagging for tag noise reduction"
  },

  "publications": [
    {
      "id": 0,
      "name": "Show Me the Instruments: Musical Instrument Retrieval From Mixture Audio",
      "webUrl": "https://arxiv.org/abs/2211.07951",
      "repoUrl": "https://github.com/minju0821/musical_instrument_retrieval",
      "author": "Kyungsu Kim*, Minju Park*, Haesun Joung*, Yunkee Chae, Yeongbeom Hong, Seonghyeon Go and Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023) (Poster)",
      "stack": ["Musical Instrument Retrieval", "Proposed Dataset:NLakh"],
      "abstract": "As digital music production has become mainstream, the selection of appropriate virtual instruments plays a crucial role in determining the quality of music. To search the musical instrument samples or virtual instruments that make one's desired sound, music producers use their ears to listen and compare each instrument sample in their collection, which is time-consuming and inefficient. In this paper, we call this task as Musical Instrument Retrieval and propose a method for retrieving desired musical instruments using reference music mixture as a query. The proposed model consists of the Single-Instrument Encoder and the Multi-Instrument Encoder, both based on convolutional neural networks. The Single-Instrument Encoder is trained to classify the instruments used in single-track audio, and we take its penultimate layer's activation as the instrument embedding. The Multi-Instrument Encoder is trained to estimate multiple instrument embeddings using the instrument embeddings computed by the Single-Instrument Encoder as a set of target embeddings. For more generalized training and realistic evaluation, we also propose a new dataset called Nlakh. Experimental results showed that the Single-Instrument Encoder was able to learn the mapping from the audio signal of unseen instruments to the instrument embedding space and the Multi-Instrument Encoder was able to extract multiple embeddings from the mixture of music and retrieve the desired instruments successfully."
    },
    {
      "id": 1,
      "name": "Towards a New Interface for Music Listening: A User Experience Study on YouTube",
      "webUrl": "https://arxiv.org/abs/2307.14718",
      "author": "Ahyeon Choi, Eunsik Shin, Haesun Joung*, Joongseek Lee and Kyogu Lee",
      "conference": "24th International Society for Music Information Retrieval Conference (ISMIR 2023) (Oral, Poster)",
      "stack": ["Music", "HCI"],
      "abstract": "In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from typical music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, despite the increasing popularity of using YouTube as a platform for music consumption, there is still a lack of comprehensive research on this phenomenon. As independent researchers unaffiliated with YouTube, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week to investigate its usability and interface satisfaction. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. These findings have wider implications beyond YouTube and could inform enhancements in other music streaming services as well."
    },
    {
      "id": 2,
      "name": "Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training",
      "webUrl": "https://arxiv.org/abs/2401.15323",
      "author": "Haesun Joung and Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024) (Poster)",
      "stack": ["Music Auto-Tagging", "Domain Adversarial Training"],
      "abstract": "Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance, underscoring its robust generalization capabilities and broad applicability."
    },
    {
      "id": 3,
      "name": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "webUrl": "https://arxiv.org/abs/2502.08939",
      "author": "Kyungsu Kim, Junghyun Koo, Sungho Lee, Haesun Joung*, Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025) (Poster)",
      "stack": ["Music Auto-Tagging", "Domain Adversarial Training"],
      "abstract": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth"
    }
  ],

  "teachingExperience": {
    "id": 0,
    "name": "Teaching Assistant",
    "lecture": "Machine Listening (SNU M2680.002400)",
    "period": ["2024. 09", "2024. 12"]
  },

  "Music": [
    {
      "id": 0,
      "name": "첫번째 수상",
      "date": "2019. 07. 28",
      "organizer": "수여기관",
      "description": "첫번째 수상에 대한 간략한 설명"
    },
    {
      "id": 1,
      "name": "첫번째 수상",
      "date": "2019. 07. 28",
      "organizer": "수여기관",
      "description": "첫번째 수상에 대한 간략한 설명"
    }
  ]

}
