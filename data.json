{
  "resumeTitle": {
    "title": "Haesun Joung"
  },

  "information": {
    "name": "정해선",
    "contact": [
      { "id": 0, "name": "Email", "href": "gotjs3841@snu.ac.kr", "isEmail": true },
      { "id": 1, "name": "Github", "href": "https://github.com/Haessun" },
      { "id": 2, "name": "GoogleScholar", "href": "https://scholar.google.com/citations?user=yV8xVKoAAAAJ&hl=en" },
      { "id": 3, "name": "LinkedIn", "href": "https://www.linkedin.com/in/해선-정-481b86175/"}
    ]
  },

  "education": [
    {
      "id": 0,
      "name": "Kaywon Arts High School",
      "period": ["2013. 03", "2016. 02"],
      "description": "Major in Classical Music Composition"
    },
    {
      "id": 1,
      "name": "B.S. in Musicology and Computer Science & Engineering @ Seoul National University",
      "period": ["2016. 03", "2021. 08"]
    },
    {
      "id": 2,
      "name": "M.S. in Intelligence and Information @ Seoul National University",
      "period": ["2021. 09", "2023. 08"],
      "thesis": "Thesis: Music Auto-tagging in Multimedia Content using Robust Music Representation Learned via Domain Adversarial Training"
    },
    {
      "id": 3,
      "name": "Ph.D. in Intelligence and Information @ Seoul National University",
      "period": ["2023. 09", "Recent"],
      "researchArea": "Music Captioning via Reinforcement Learning, Musical Instrument/Timbre Retrieval, Music HCI, Music Perception & Cognition"
    }
  ],

  "workExperience": [
      {
        "id": 0,
        "name": "Research Intern @ NAVER VIBE AI",
        "period": ["2025. 01", "2025. 02"],
        "description": "Music genre auto-tagging for tag noise reduction"
      },
      {
        "id": 1,
        "name": "Research Intern @ SonyCSL",
        "period": ["2025. 05", "2025. 07"],
        "description": "Perceptual and Multi-dimensional Music Similarity-Based Retrieval"
      }
  ],

  "publications": [
    {
      "id": 0,
      "name": "Show Me the Instruments: Musical Instrument Retrieval From Mixture Audio",
      "webUrl": "https://arxiv.org/abs/2211.07951",
      "repoUrl": "https://github.com/minju0821/musical_instrument_retrieval",
      "author": "Kyungsu Kim*, Minju Park*, Haesun Joung*, Yunkee Chae, Yeongbeom Hong, Seonghyeon Go and Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023) (Poster)",
      "stack": ["Musical Instrument Retrieval", "Proposed Dataset:NLakh"],
      "abstract": "As digital music production has become mainstream, the selection of appropriate virtual instruments plays a crucial role in determining the quality of music. To search the musical instrument samples or virtual instruments that make one's desired sound, music producers use their ears to listen and compare each instrument sample in their collection, which is time-consuming and inefficient. In this paper, we call this task as Musical Instrument Retrieval and propose a method for retrieving desired musical instruments using reference music mixture as a query. The proposed model consists of the Single-Instrument Encoder and the Multi-Instrument Encoder, both based on convolutional neural networks. The Single-Instrument Encoder is trained to classify the instruments used in single-track audio, and we take its penultimate layer's activation as the instrument embedding. The Multi-Instrument Encoder is trained to estimate multiple instrument embeddings using the instrument embeddings computed by the Single-Instrument Encoder as a set of target embeddings. For more generalized training and realistic evaluation, we also propose a new dataset called Nlakh. Experimental results showed that the Single-Instrument Encoder was able to learn the mapping from the audio signal of unseen instruments to the instrument embedding space and the Multi-Instrument Encoder was able to extract multiple embeddings from the mixture of music and retrieve the desired instruments successfully."
    },
    {
      "id": 1,
      "name": "Towards a New Interface for Music Listening: A User Experience Study on YouTube",
      "webUrl": "https://arxiv.org/abs/2307.14718",
      "author": "Ahyeon Choi, Eunsik Shin, Haesun Joung, Joongseek Lee and Kyogu Lee",
      "conference": "24th International Society for Music Information Retrieval Conference (ISMIR 2023) (Oral, Poster)",
      "stack": ["Music", "HCI"],
      "abstract": "In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from typical music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, despite the increasing popularity of using YouTube as a platform for music consumption, there is still a lack of comprehensive research on this phenomenon. As independent researchers unaffiliated with YouTube, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week to investigate its usability and interface satisfaction. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. These findings have wider implications beyond YouTube and could inform enhancements in other music streaming services as well."
    },
    {
      "id": 2,
      "name": "Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training",
      "webUrl": "https://arxiv.org/abs/2401.15323",
      "author": "Haesun Joung and Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024) (Poster)",
      "stack": ["Music Auto-Tagging", "Domain Adversarial Training"],
      "abstract": "Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance, underscoring its robust generalization capabilities and broad applicability."
    },
    {
      "id": 3,
      "name": "The Impact of Visual Information in Speech Perception for Individuals with Hearing Loss: A Mini Review",
      "webUrl": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1399084/full",
      "author": "Ahyeon Choi, Hayoon Kim, Mina Jo, Subeen Kim, Haesun Joung, Inyong Choi, Kyogu Lee",
      "conference": "Frontiers in Psychology 15, 1399084 (2024)",
      "stack": ["Speech Perception", "Hearing Loss", "Cochlear Implant", "Visual Information", "Cross-Modal Plasticity", "Multisensory Integration", "Age-Related Variation", "Linguistic Level"],
      "abstract": "This review examines how visual information enhances speech perception in individuals with hearing loss, focusing on the impact of age, linguistic stimuli, and specific hearing loss factors on the effectiveness of audiovisual (AV) integration. While existing studies offer varied and sometimes conflicting findings regarding the use of visual cues, our analysis shows that these key factors can distinctly shape AV speech perception outcomes. For instance, younger individuals and those who receive early intervention tend to benefit more from visual cues, particularly when linguistic complexity is lower. Additionally, languages with dense phoneme spaces demonstrate a higher dependency on visual information, underscoring the importance of tailoring rehabilitation strategies to specific linguistic contexts. By considering these influences, we highlight areas where understanding is still developing and suggest how personalized rehabilitation strategies and supportive systems could be tailored to better meet individual needs. Furthermore, this review brings attention to important aspects that warrant further investigation, aiming to refine theoretical models and contribute to more effective, customized approaches to hearing rehabilitation."
    },
    {
      "id": 4,
      "name": "Optimizing Music Captioning with Reinforcement Learning and Retrieval-Augmented Methods",
      "webUrl": "https://ismir2024program.ismir.net/lbd_429.html",
      "author": "Haesun Joung, Jinwoo Lee, Kyogu Lee",
      "conference": "25th International Society for Music Information Retrieval Conference (ISMIR 2024) (Late-Breaking Demo)",
      "stack": ["Music Captioning", "Reinforcement Learning", "Retrieval-Augmented Generation"],
      "abstract": "Music captioning, the task of generating natural language descriptions for musical audio, often faces challenges due to the limited availability of paired music-caption datasets, largely caused by copyright constraints. In this study, we explore a Retrieval-Augmented Generation (RAG) approach to address this issue by retrieving relevant captions from a large datastore using an efficient similarity search. To improve the alignment between generated captions and audio, we incorporate semantic loss, which uses a reward function based on reinforcement learning to optimize the semantic consistency between the generated caption and the reference caption. Additionally, we employ Llama-3.1, a large language model, to enhance the quality of the generated captions. This method enhances semantic consistency and improves the linguistic accuracy of generated captions, contributing to advancements in music information retrieval (MIR) applications."
    },
    {
      "id": 5,
      "name": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "webUrl": "https://arxiv.org/abs/2502.08939",
      "author": "Kyungsu Kim, Junghyun Koo, Sungho Lee, Haesun Joung, Kyogu Lee",
      "conference": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025) (Poster)",
      "stack": ["Music Auto-Tagging", "Domain Adversarial Training"],
      "abstract": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth"
    },
    {
      "id": 6,
      "name": "Exploring the Speech-to-Song Illusion: A Comparative Study of Standard Korean and Dialects",
      "webUrl": "https://escholarship.org/uc/item/59n9j0gq",
      "author": "Haesun Joung, Ahyeon Choi, Kyogu Lee",
      "conference": "Cognitive Science Society (CogSci 2025) (Poster, Full paper publication)",
      "stack": ["Auditory Illusion", "Speech-to-Song Illusion", "Tonal and Non-Tonal Languages", "Korean Dialects"],
      "abstract": "The Speech-to-Song Illusion (STS) phenomenon, where repeated short speech utterances transform into perceived song, has drawn attention to its underlying mechanisms and cross-linguistic differences. This study examines the STS effects among Korean speakers, comparing standard Korean (non-tonal) and dialects such as Gyeongsang and Jeju (tonal), which exhibit varying levels of linguistic tonal features. Participants (N = 60), evenly divided between standard and dialect users, evaluated 180 auditory stimuli comprising standard Korean, Gyeongsang, and Jeju utterances under controlled repetition conditions. Results revealed significant STS effects across all groups and stimuli, with stronger effects observed for dialectal stimuli, particularly Jeju, compared to standard Korean. Interestingly, differences between standard and dialect speaker groups in STS perception were not statistically significant, suggesting that exposure to diverse linguistic environments, facilitated by modern Korean media, may homogenize perceptual responses to tonal variations. The study highlights the influence of tonal and rhythmic elements in STS perception and underscores the cultural and linguistic uniqueness of Korean as a fertile ground for exploring auditory illusions. This research contributes to understanding the interplay of linguistic and perceptual factors in STS and opens avenues for cross-cultural comparisons and neuroscientific investigations of auditory illusions."
    }
  ],

  "teachingExperience": [
    {
      "id": 0,
      "name": "Teaching Assistant",
      "lecture": "Machine Listening (SNU M2680.002400)",
      "period": ["2024. 09", "2024. 12"]
    }
  ],

  "professionalSocieties": {
    "review": [
      "International Computer Music Conference (ICMC 2025)"
    ],
    "membership": [
      "Cognitive Science Society (CSS)",
      "IEEE Signal Processing Society (IEEE SPS)",
      "International Speech Communication Association (ISCA)"
    ]
  },

  "Music": [
    {
      "id": 0,
      "name": "첫번째 수상",
      "date": "2019. 07. 28",
      "organizer": "수여기관",
      "description": "첫번째 수상에 대한 간략한 설명"
    },
    {
      "id": 1,
      "name": "첫번째 수상",
      "date": "2019. 07. 28",
      "organizer": "수여기관",
      "description": "첫번째 수상에 대한 간략한 설명"
    }
  ]

}
